{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZPX-N9HEJ1T",
        "outputId": "b0c1c33a-d1c3-4faa-ac14-c5e403c9796f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f0O0T5XHECHO"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Data Preprocessing \n",
        "def check_tags_size(tags_list):\n",
        "    assert len(tags_list) == 1, f\"Expected list of size 1, but got {len(tags_list)}\"\n",
        "\n",
        "def check_clickbait_size(clickbait_list):\n",
        "    assert len(clickbait_list) == 1, f\"Expected list of size 1, but got {len(clickbait_list)}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocessData(fileName):\n",
        "  data = []\n",
        "  # Load data\n",
        "  with open(fileName, \"r\") as file:\n",
        "    for line in file:\n",
        "      data.append(json.loads(line))\n",
        "\n",
        "  # Convert to DataFrame\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  # Take only relevant columns to create training data\n",
        "  df = df[[\"postText\", \"targetParagraphs\", \"targetTitle\", \"tags\"]]\n",
        "\n",
        "  # Convert tags to numbers\n",
        "  df['tags'].apply(check_tags_size)\n",
        "  df['tags'] = df['tags'].apply(lambda tags_list: tags_list[0])\n",
        "  tag_mapping = {'phrase': 0, 'passage': 1, 'multi': 2}\n",
        "  df['tags'] = df['tags'].map(tag_mapping)\n",
        "\n",
        "  # Filter out data having multi tag\n",
        "  df = df[df['tags'] != 2]\n",
        "\n",
        "  # Concat targetTitle and targetParagraphs\n",
        "  df['text'] = df['targetTitle'] + ' ' + df['targetParagraphs'].apply(' '.join)\n",
        "\n",
        "  # Rename postText to clickbait to make things more relevant\n",
        "  df.rename(columns={'postText': 'clickbait'}, inplace=True)\n",
        "\n",
        "  df['clickbait'].apply(check_clickbait_size)\n",
        "  df['clickbait'] = df['clickbait'].apply(lambda clickbait_list: clickbait_list[0])\n",
        "\n",
        "  # Drop targetTitle, targetParagraphs as they are no more required\n",
        "  df.drop(['targetTitle', 'targetParagraphs'], axis=1, inplace=True)\n",
        "\n",
        "  # Clean Data\n",
        "  df['clickbait'] = df['clickbait'].apply(clean_text)\n",
        "  df['text'] = df['text'].apply(clean_text)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mVkldmsTOdTW",
        "outputId": "bbd29c08-9818-4142-dc9c-773d20e811d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           clickbait  tags  \\\n",
              "0  wes welker wanted dinner tom brady patriot qb ...     1   \n",
              "1             nasa set date full recovery ozone hole     0   \n",
              "2                       make employee happy paycheck     0   \n",
              "4  perfect way cook rice perfectly fluffy never s...     0   \n",
              "5  happens new airpods get lost stolen apple anyt...     1   \n",
              "\n",
              "                                                text  \n",
              "0  wes welker wanted dinner tom brady patriot qb ...  \n",
              "1  hole ozone layer expected make full recovery n...  \n",
              "2  intellectual stimulation trump money employee ...  \n",
              "4  revealed perfect way cook rice perfectly fluff...  \n",
              "5  happens apple airpods get lost stolen one bigg...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-181ea1c2-e15d-4cfa-9ff9-263048c7bd98\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clickbait</th>\n",
              "      <th>tags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wes welker wanted dinner tom brady patriot qb ...</td>\n",
              "      <td>1</td>\n",
              "      <td>wes welker wanted dinner tom brady patriot qb ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nasa set date full recovery ozone hole</td>\n",
              "      <td>0</td>\n",
              "      <td>hole ozone layer expected make full recovery n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>make employee happy paycheck</td>\n",
              "      <td>0</td>\n",
              "      <td>intellectual stimulation trump money employee ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>perfect way cook rice perfectly fluffy never s...</td>\n",
              "      <td>0</td>\n",
              "      <td>revealed perfect way cook rice perfectly fluff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>happens new airpods get lost stolen apple anyt...</td>\n",
              "      <td>1</td>\n",
              "      <td>happens apple airpods get lost stolen one bigg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-181ea1c2-e15d-4cfa-9ff9-263048c7bd98')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-181ea1c2-e15d-4cfa-9ff9-263048c7bd98 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-181ea1c2-e15d-4cfa-9ff9-263048c7bd98');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Training data preprocessing\n",
        "trainingFileName = \"/content/drive/MyDrive/Colab Notebooks/webis-clickbait-22/train.jsonl\"\n",
        "validationFileName = \"/content/drive/MyDrive/Colab Notebooks/webis-clickbait-22/validation.jsonl\"\n",
        "train_df = preprocessData(trainingFileName)\n",
        "validation_df = preprocessData(validationFileName)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVohlR6KTG2L",
        "outputId": "ed7d3f03-ef65-473b-d469-b3618722bf9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Clickbait - Number of features =  28520\n",
            "Train Text - Number of features =  1167649\n",
            "Validation Clickbait - Number of features =  28520\n",
            "Validation Text - Number of features =  1167649\n"
          ]
        }
      ],
      "source": [
        "# Generating TFIDF for both clickbait and text column \n",
        "def generateTfidf(train_df, validation_df):\n",
        "  # Create a TfidfVectorizer for trigrams\n",
        "  clickbait_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "  text_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "  # Fit and transform the text data\n",
        "  clickbait_tfidf = clickbait_tfidf_vectorizer.fit_transform(train_df[\"clickbait\"])\n",
        "  text_tfidf = text_tfidf_vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "  clickbait_tfidf_validation = clickbait_tfidf_vectorizer.transform(validation_df[\"clickbait\"])\n",
        "  text_tfidf_validation = text_tfidf_vectorizer.transform(validation_df[\"text\"])\n",
        "\n",
        "  print(\"Train Clickbait - Number of features = \", len(clickbait_tfidf_vectorizer.get_feature_names_out()))\n",
        "  print(\"Train Text - Number of features = \", len(text_tfidf_vectorizer.get_feature_names_out()))\n",
        "\n",
        "  print(\"Validation Clickbait - Number of features = \", len(clickbait_tfidf_vectorizer.get_feature_names_out()))\n",
        "  print(\"Validation Text - Number of features = \", len(text_tfidf_vectorizer.get_feature_names_out()))\n",
        "  return clickbait_tfidf, text_tfidf, clickbait_tfidf_validation, text_tfidf_validation\n",
        "\n",
        "train_clickbait_tfidf, train_text_tfidf, validation_clickbait_tfidf, validation_text_tfidf = generateTfidf(train_df, validation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ZuqQrwZ-1s",
        "outputId": "7f260332-c89f-4313-dc8f-7d0b7f2773c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_clickbait_length  96\n"
          ]
        }
      ],
      "source": [
        "# Generating one hot encoded POS for clickbait \n",
        "def generatePos(train_df, validation_df):\n",
        "  train_df[\"clickbait_pos\"] = train_df[\"clickbait\"].apply(lambda x: [tag for _, tag in pos_tag(word_tokenize(x))])\n",
        "  validation_df[\"clickbait_pos\"] = validation_df[\"clickbait\"].apply(lambda x: [tag for _, tag in pos_tag(word_tokenize(x))])\n",
        "\n",
        "  # Tokenize the POS tags\n",
        "  pos_tokenizer = Tokenizer()\n",
        "  pos_tokenizer.fit_on_texts(train_df['clickbait_pos'])\n",
        "\n",
        "  # Convert the POS tags to integer sequences\n",
        "  train_df['clickbait_pos_encoded'] = train_df['clickbait_pos'].apply(lambda x: pos_tokenizer.texts_to_sequences([x])[0]) \n",
        "  validation_df['clickbait_pos_encoded'] = validation_df['clickbait_pos'].apply(lambda x: pos_tokenizer.texts_to_sequences([x])[0]) \n",
        "\n",
        "  # Pad the sequences to a fixed length\n",
        "  max_clickbait_length = train_df['clickbait'].apply(len).max() \n",
        "  print(\"max_clickbait_length \", max_clickbait_length)\n",
        "  train_df['clickbait_pos_padded'] = pad_sequences(train_df['clickbait_pos_encoded'], maxlen=max_clickbait_length, padding='post', value=0).tolist() \n",
        "  validation_df['clickbait_pos_padded'] = pad_sequences(validation_df['clickbait_pos_encoded'], maxlen=max_clickbait_length, padding='post', value=0).tolist() \n",
        "\n",
        "generatePos(train_df, validation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujf9CWlQ9pq-",
        "outputId": "111e6348-ed89-43ad-a029-ee8fbc884b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2641, 496)\n",
            "(2641, 496)\n"
          ]
        }
      ],
      "source": [
        "def reduce_dimensions(train_tfidf, validation_tfidf, n_components=100):\n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "    train_reduced = svd.fit_transform(train_tfidf)\n",
        "    validation_reduced = svd.transform(validation_tfidf)\n",
        "    return train_reduced, validation_reduced\n",
        "\n",
        "# Set the desired number of components (dimensions) to keep\n",
        "n_components = 200\n",
        "\n",
        "train_clickbait_reduced, validation_clickbait_reduced = reduce_dimensions(train_clickbait_tfidf, validation_clickbait_tfidf, n_components)\n",
        "train_text_reduced, validation_text_reduced = reduce_dimensions(train_text_tfidf, validation_text_tfidf, n_components)\n",
        "\n",
        "def get_combined_features_reduced(df, clickbait_reduced, text_reduced):\n",
        "    clickbait_pos_padded_array = np.array(df['clickbait_pos_padded'].tolist())\n",
        "    combined_features = np.hstack([clickbait_reduced, text_reduced, clickbait_pos_padded_array])\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_combined_features = scaler.fit_transform(combined_features)\n",
        "    return scaled_combined_features\n",
        "\n",
        "X_train = get_combined_features_reduced(train_df, train_clickbait_reduced, train_text_reduced)\n",
        "X_validation = get_combined_features_reduced(validation_df, validation_clickbait_reduced, validation_text_reduced)\n",
        "print(X_train.shape)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "y_train = train_df[\"tags\"]\n",
        "y_validation = validation_df[\"tags\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbuy4yAfs5Zb"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_validation_tensor = torch.tensor(X_validation, dtype=torch.float32)\n",
        "y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNDCaKkzJJj"
      },
      "source": [
        "## Logistic Regression Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYPlyg_cy6FU",
        "outputId": "b296edf4-1567-44c7-8f43-37ab73d8e914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, Training F1: 0.6241, Training Accuracy: 0.6354, Validation F1: 0.5599, Validation Accuracy: 0.5890\n",
            "Epoch: 100, Training F1: 0.5636, Training Accuracy: 0.6058, Validation F1: 0.5229, Validation Accuracy: 0.5784\n",
            "Epoch: 150, Training F1: 0.5626, Training Accuracy: 0.6191, Validation F1: 0.5192, Validation Accuracy: 0.5693\n",
            "Epoch: 200, Training F1: 0.6089, Training Accuracy: 0.6445, Validation F1: 0.5700, Validation Accuracy: 0.5936\n",
            "Epoch: 250, Training F1: 0.6704, Training Accuracy: 0.6766, Validation F1: 0.6000, Validation Accuracy: 0.6195\n",
            "Epoch: 300, Training F1: 0.6794, Training Accuracy: 0.6838, Validation F1: 0.6494, Validation Accuracy: 0.6499\n",
            "Epoch: 350, Training F1: 0.6489, Training Accuracy: 0.6676, Validation F1: 0.6182, Validation Accuracy: 0.6256\n",
            "Epoch: 400, Training F1: 0.6937, Training Accuracy: 0.6944, Validation F1: 0.6525, Validation Accuracy: 0.6530\n",
            "Epoch: 450, Training F1: 0.6889, Training Accuracy: 0.6891, Validation F1: 0.6306, Validation Accuracy: 0.6347\n",
            "Epoch: 500, Training F1: 0.6981, Training Accuracy: 0.6982, Validation F1: 0.6547, Validation Accuracy: 0.6560\n",
            "Epoch: 550, Training F1: 0.6973, Training Accuracy: 0.6975, Validation F1: 0.6548, Validation Accuracy: 0.6560\n",
            "Epoch: 600, Training F1: 0.6990, Training Accuracy: 0.6994, Validation F1: 0.6582, Validation Accuracy: 0.6591\n",
            "Epoch: 650, Training F1: 0.7012, Training Accuracy: 0.7024, Validation F1: 0.6560, Validation Accuracy: 0.6560\n",
            "Epoch: 700, Training F1: 0.6854, Training Accuracy: 0.6922, Validation F1: 0.6548, Validation Accuracy: 0.6560\n",
            "Epoch: 750, Training F1: 0.6988, Training Accuracy: 0.6990, Validation F1: 0.6599, Validation Accuracy: 0.6606\n",
            "Epoch: 800, Training F1: 0.7039, Training Accuracy: 0.7047, Validation F1: 0.6358, Validation Accuracy: 0.6408\n",
            "Epoch: 850, Training F1: 0.7026, Training Accuracy: 0.7047, Validation F1: 0.6528, Validation Accuracy: 0.6530\n",
            "Epoch: 900, Training F1: 0.7009, Training Accuracy: 0.7009, Validation F1: 0.6560, Validation Accuracy: 0.6575\n",
            "Epoch: 950, Training F1: 0.7037, Training Accuracy: 0.7039, Validation F1: 0.6492, Validation Accuracy: 0.6514\n",
            "Epoch: 1000, Training F1: 0.6863, Training Accuracy: 0.6944, Validation F1: 0.6485, Validation Accuracy: 0.6514\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.sigmoid(self.linear(x))\n",
        "        return out\n",
        "\n",
        "# Set up hyperparameters\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoader for mini-batch processing\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create DataLoader for validation dataset\n",
        "validation_dataset = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = LogisticRegression(input_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model and evaluate on validation dataset\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(x)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate and print accuracy on training dataset\n",
        "    # Calculate and print Macro-averaged F1 and accuracy on training dataset\n",
        "    with torch.no_grad():\n",
        "        train_outputs = model(X_train_tensor)\n",
        "        train_preds = (train_outputs > 0.5).float()\n",
        "        train_acc = accuracy_score(y_train_tensor, train_preds)\n",
        "        train_f1 = f1_score(y_train_tensor, train_preds, average='macro')\n",
        "    \n",
        "    # Calculate and print Macro-averaged F1 and accuracy on validation dataset\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "        y_val_true = []\n",
        "        y_val_preds = []\n",
        "\n",
        "        for x_val, y_val in validation_loader:\n",
        "            val_outputs = model(x_val)\n",
        "            val_preds = (val_outputs > 0.5).float()\n",
        "            correct_preds += (val_preds == y_val).sum().item()\n",
        "            total_preds += x_val.size(0)\n",
        "            y_val_true.extend(y_val.tolist())\n",
        "            y_val_preds.extend(val_preds.tolist())\n",
        "\n",
        "        val_acc = correct_preds / total_preds\n",
        "        val_f1 = f1_score(y_val_true, y_val_preds, average='macro')\n",
        "        model.train()\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Training F1: {train_f1:.4f}, Training Accuracy: {train_acc:.4f}, Validation F1: {val_f1:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FNOaF2c1hKH"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDahujpauToh",
        "outputId": "4f461f65-f594-4fee-f82f-e2e8e23bda16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training F1 (Naive Bayes): 0.4875\n",
            "Training Accuracy (Naive Bayes): 0.5782\n",
            "Validation F1 (Naive Bayes): 0.5335\n",
            "Validation Accuracy (Naive Bayes): 0.5693\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and training accuracy\n",
        "train_preds_NB = naive_bayes_classifier.predict(X_train)\n",
        "train_acc_NB = accuracy_score(y_train, train_preds_NB)\n",
        "train_f1_NB = f1_score(y_train, train_preds_NB, average='macro')\n",
        "print(f\"Training F1 (Naive Bayes): {train_f1_NB:.4f}\")\n",
        "print(f\"Training Accuracy (Naive Bayes): {train_acc_NB:.4f}\")\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and validation accuracy\n",
        "validation_preds_NB = naive_bayes_classifier.predict(X_validation)\n",
        "validation_acc_NB = accuracy_score(y_validation, validation_preds_NB)\n",
        "validation_f1_NB = f1_score(y_validation, validation_preds_NB, average='macro')\n",
        "print(f\"Validation F1 (Naive Bayes): {validation_f1_NB:.4f}\")\n",
        "print(f\"Validation Accuracy (Naive Bayes): {validation_acc_NB:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQgifZ8G4KWV"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJyDbd6q10gW",
        "outputId": "e080795d-24bb-4508-a136-a45e0a541542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training F1 (SVM): 0.7427\n",
            "Training Accuracy (SVM): 0.7429\n",
            "Validation F1 (SVM): 0.5251\n",
            "Validation Accuracy (SVM): 0.5784\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', C=1)\n",
        "\n",
        "# Train the SVM classifier using the scaled training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and training accuracy\n",
        "train_preds_svm = svm_classifier.predict(X_train)\n",
        "train_acc_svm = accuracy_score(y_train, train_preds_svm)\n",
        "train_f1_svm = f1_score(y_train, train_preds_svm, average='macro')\n",
        "print(f\"Training F1 (SVM): {train_f1_svm:.4f}\")\n",
        "print(f\"Training Accuracy (SVM): {train_acc_svm:.4f}\")\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and validation accuracy\n",
        "validation_preds_svm = svm_classifier.predict(X_validation)\n",
        "validation_acc_svm = accuracy_score(y_validation, validation_preds_svm)\n",
        "validation_f1_svm = f1_score(y_validation, validation_preds_svm, average='macro')\n",
        "print(f\"Validation F1 (SVM): {validation_f1_svm:.4f}\")\n",
        "print(f\"Validation Accuracy (SVM): {validation_acc_svm:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
