{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZPX-N9HEJ1T",
        "outputId": "1af950cc-0caf-4fad-fad8-519392eafd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f0O0T5XHECHO"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Data Preprocessing \n",
        "def check_tags_size(tags_list):\n",
        "    assert len(tags_list) == 1, f\"Expected list of size 1, but got {len(tags_list)}\"\n",
        "\n",
        "def check_clickbait_size(clickbait_list):\n",
        "    assert len(clickbait_list) == 1, f\"Expected list of size 1, but got {len(clickbait_list)}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocessData(fileName):\n",
        "  data = []\n",
        "  # Load data\n",
        "  with open(fileName, \"r\") as file:\n",
        "    for line in file:\n",
        "      data.append(json.loads(line))\n",
        "\n",
        "  # Convert to DataFrame\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  # Take only relevant columns to create training data\n",
        "  df = df[[\"postText\", \"targetParagraphs\", \"targetTitle\", \"tags\"]]\n",
        "\n",
        "  # Convert tags to numbers\n",
        "  df['tags'].apply(check_tags_size)\n",
        "  df['tags'] = df['tags'].apply(lambda tags_list: tags_list[0])\n",
        "  tag_mapping = {'phrase': 0, 'passage': 1, 'multi': 2}\n",
        "  df['tags'] = df['tags'].map(tag_mapping)\n",
        "\n",
        "  # Filter out data having multi tag\n",
        "  df = df[df['tags'] != 2]\n",
        "\n",
        "  # Concat targetTitle and targetParagraphs\n",
        "  df['text'] = df['targetTitle'] + ' ' + df['targetParagraphs'].apply(' '.join)\n",
        "\n",
        "  # Rename postText to clickbait to make things more relevant\n",
        "  df.rename(columns={'postText': 'clickbait'}, inplace=True)\n",
        "\n",
        "  df['clickbait'].apply(check_clickbait_size)\n",
        "  df['clickbait'] = df['clickbait'].apply(lambda clickbait_list: clickbait_list[0])\n",
        "\n",
        "  # Drop targetTitle, targetParagraphs as they are no more required\n",
        "  df.drop(['targetTitle', 'targetParagraphs'], axis=1, inplace=True)\n",
        "\n",
        "  # Clean Data\n",
        "  df['clickbait'] = df['clickbait'].apply(clean_text)\n",
        "  df['text'] = df['text'].apply(clean_text)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mVkldmsTOdTW",
        "outputId": "338e47d0-b14e-4db9-ec82-736e0abba5dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           clickbait  tags  \\\n",
              "0  wes welker wanted dinner tom brady patriot qb ...     1   \n",
              "1             nasa set date full recovery ozone hole     0   \n",
              "2                       make employee happy paycheck     0   \n",
              "4  perfect way cook rice perfectly fluffy never s...     0   \n",
              "5  happens new airpods get lost stolen apple anyt...     1   \n",
              "\n",
              "                                                text  \n",
              "0  wes welker wanted dinner tom brady patriot qb ...  \n",
              "1  hole ozone layer expected make full recovery n...  \n",
              "2  intellectual stimulation trump money employee ...  \n",
              "4  revealed perfect way cook rice perfectly fluff...  \n",
              "5  happens apple airpods get lost stolen one bigg...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-638882c6-5247-43be-a428-fa61b37994f5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clickbait</th>\n",
              "      <th>tags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wes welker wanted dinner tom brady patriot qb ...</td>\n",
              "      <td>1</td>\n",
              "      <td>wes welker wanted dinner tom brady patriot qb ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nasa set date full recovery ozone hole</td>\n",
              "      <td>0</td>\n",
              "      <td>hole ozone layer expected make full recovery n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>make employee happy paycheck</td>\n",
              "      <td>0</td>\n",
              "      <td>intellectual stimulation trump money employee ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>perfect way cook rice perfectly fluffy never s...</td>\n",
              "      <td>0</td>\n",
              "      <td>revealed perfect way cook rice perfectly fluff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>happens new airpods get lost stolen apple anyt...</td>\n",
              "      <td>1</td>\n",
              "      <td>happens apple airpods get lost stolen one bigg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-638882c6-5247-43be-a428-fa61b37994f5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-638882c6-5247-43be-a428-fa61b37994f5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-638882c6-5247-43be-a428-fa61b37994f5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Training data preprocessing\n",
        "trainingFileName = \"/content/drive/MyDrive/Colab Notebooks/webis-clickbait-22/train.jsonl\"\n",
        "validationFileName = \"/content/drive/MyDrive/Colab Notebooks/webis-clickbait-22/validation.jsonl\"\n",
        "train_df = preprocessData(trainingFileName)\n",
        "validation_df = preprocessData(validationFileName)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVohlR6KTG2L",
        "outputId": "3dfa684b-9031-4fbb-a28e-560159ca6771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Clickbait - Number of features =  28520\n",
            "Train Text - Number of features =  1167649\n",
            "Validation Clickbait - Number of features =  28520\n",
            "Validation Text - Number of features =  1167649\n"
          ]
        }
      ],
      "source": [
        "# Generating TFIDF for both clickbait and text column \n",
        "def generateTfidf(train_df, validation_df):\n",
        "  # Create a TfidfVectorizer for trigrams\n",
        "  clickbait_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "  text_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "  # Fit and transform the text data\n",
        "  clickbait_tfidf = clickbait_tfidf_vectorizer.fit_transform(train_df[\"clickbait\"])\n",
        "  text_tfidf = text_tfidf_vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "  clickbait_tfidf_validation = clickbait_tfidf_vectorizer.transform(validation_df[\"clickbait\"])\n",
        "  text_tfidf_validation = text_tfidf_vectorizer.transform(validation_df[\"text\"])\n",
        "\n",
        "  print(\"Train Clickbait - Number of features = \", len(clickbait_tfidf_vectorizer.get_feature_names_out()))\n",
        "  print(\"Train Text - Number of features = \", len(text_tfidf_vectorizer.get_feature_names_out()))\n",
        "\n",
        "  print(\"Validation Clickbait - Number of features = \", len(clickbait_tfidf_vectorizer.get_feature_names_out()))\n",
        "  print(\"Validation Text - Number of features = \", len(text_tfidf_vectorizer.get_feature_names_out()))\n",
        "  return clickbait_tfidf, text_tfidf, clickbait_tfidf_validation, text_tfidf_validation\n",
        "\n",
        "train_clickbait_tfidf, train_text_tfidf, validation_clickbait_tfidf, validation_text_tfidf = generateTfidf(train_df, validation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ZuqQrwZ-1s",
        "outputId": "75d0b6cf-9dfd-4799-e5e4-fb7f2749f8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_clickbait_length  96\n"
          ]
        }
      ],
      "source": [
        "# Generating one hot encoded POS for clickbait \n",
        "def generatePos(train_df, validation_df):\n",
        "  train_df[\"clickbait_pos\"] = train_df[\"clickbait\"].apply(lambda x: [tag for _, tag in pos_tag(word_tokenize(x))])\n",
        "  validation_df[\"clickbait_pos\"] = validation_df[\"clickbait\"].apply(lambda x: [tag for _, tag in pos_tag(word_tokenize(x))])\n",
        "\n",
        "  # Tokenize the POS tags\n",
        "  pos_tokenizer = Tokenizer()\n",
        "  pos_tokenizer.fit_on_texts(train_df['clickbait_pos'])\n",
        "\n",
        "  # Convert the POS tags to integer sequences\n",
        "  train_df['clickbait_pos_encoded'] = train_df['clickbait_pos'].apply(lambda x: pos_tokenizer.texts_to_sequences([x])[0]) \n",
        "  validation_df['clickbait_pos_encoded'] = validation_df['clickbait_pos'].apply(lambda x: pos_tokenizer.texts_to_sequences([x])[0]) \n",
        "\n",
        "  # Pad the sequences to a fixed length\n",
        "  max_clickbait_length = train_df['clickbait'].apply(len).max() \n",
        "  print(\"max_clickbait_length \", max_clickbait_length)\n",
        "  train_df['clickbait_pos_padded'] = pad_sequences(train_df['clickbait_pos_encoded'], maxlen=max_clickbait_length, padding='post', value=0).tolist() \n",
        "  validation_df['clickbait_pos_padded'] = pad_sequences(validation_df['clickbait_pos_encoded'], maxlen=max_clickbait_length, padding='post', value=0).tolist() \n",
        "\n",
        "generatePos(train_df, validation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujf9CWlQ9pq-",
        "outputId": "77177205-28f7-4d30-bb0f-d797ce464d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2641, 496)\n",
            "(2641, 496)\n"
          ]
        }
      ],
      "source": [
        "def reduce_dimensions(train_tfidf, validation_tfidf, n_components=100):\n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "    train_reduced = svd.fit_transform(train_tfidf)\n",
        "    validation_reduced = svd.transform(validation_tfidf)\n",
        "    return train_reduced, validation_reduced\n",
        "\n",
        "# Set the desired number of components (dimensions) to keep\n",
        "n_components = 200\n",
        "\n",
        "train_clickbait_reduced, validation_clickbait_reduced = reduce_dimensions(train_clickbait_tfidf, validation_clickbait_tfidf, n_components)\n",
        "train_text_reduced, validation_text_reduced = reduce_dimensions(train_text_tfidf, validation_text_tfidf, n_components)\n",
        "\n",
        "def get_combined_features_reduced(df, clickbait_reduced, text_reduced):\n",
        "    clickbait_pos_padded_array = np.array(df['clickbait_pos_padded'].tolist())\n",
        "    combined_features = np.hstack([clickbait_reduced, text_reduced, clickbait_pos_padded_array])\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_combined_features = scaler.fit_transform(combined_features)\n",
        "    return scaled_combined_features\n",
        "\n",
        "X_train = get_combined_features_reduced(train_df, train_clickbait_reduced, train_text_reduced)\n",
        "X_validation = get_combined_features_reduced(validation_df, validation_clickbait_reduced, validation_text_reduced)\n",
        "print(X_train.shape)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "y_train = train_df[\"tags\"]\n",
        "y_validation = validation_df[\"tags\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mbuy4yAfs5Zb"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_validation_tensor = torch.tensor(X_validation, dtype=torch.float32)\n",
        "y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNDCaKkzJJj"
      },
      "source": [
        "## Logistic Regression Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYPlyg_cy6FU",
        "outputId": "4b2e31cb-b333-4df9-bbe6-20c5ca4d1ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, Training F1: 0.5506, Training Accuracy: 0.6032, Validation F1: 0.4204, Validation Accuracy: 0.5373\n",
            "Epoch: 100, Training F1: 0.4804, Training Accuracy: 0.5767, Validation F1: 0.4159, Validation Accuracy: 0.5297\n",
            "Epoch: 150, Training F1: 0.6462, Training Accuracy: 0.6638, Validation F1: 0.5913, Validation Accuracy: 0.6088\n",
            "Epoch: 200, Training F1: 0.6894, Training Accuracy: 0.6895, Validation F1: 0.6370, Validation Accuracy: 0.6377\n",
            "Epoch: 250, Training F1: 0.6859, Training Accuracy: 0.6861, Validation F1: 0.6390, Validation Accuracy: 0.6393\n",
            "Epoch: 300, Training F1: 0.6779, Training Accuracy: 0.6831, Validation F1: 0.6438, Validation Accuracy: 0.6454\n",
            "Epoch: 350, Training F1: 0.6718, Training Accuracy: 0.6793, Validation F1: 0.6435, Validation Accuracy: 0.6454\n",
            "Epoch: 400, Training F1: 0.6895, Training Accuracy: 0.6929, Validation F1: 0.6433, Validation Accuracy: 0.6438\n",
            "Epoch: 450, Training F1: 0.6891, Training Accuracy: 0.6891, Validation F1: 0.6467, Validation Accuracy: 0.6484\n",
            "Epoch: 500, Training F1: 0.6937, Training Accuracy: 0.6937, Validation F1: 0.6516, Validation Accuracy: 0.6530\n",
            "Epoch: 550, Training F1: 0.6770, Training Accuracy: 0.6842, Validation F1: 0.6344, Validation Accuracy: 0.6362\n",
            "Epoch: 600, Training F1: 0.6910, Training Accuracy: 0.6952, Validation F1: 0.6508, Validation Accuracy: 0.6514\n",
            "Epoch: 650, Training F1: 0.6977, Training Accuracy: 0.6986, Validation F1: 0.6397, Validation Accuracy: 0.6454\n",
            "Epoch: 700, Training F1: 0.6994, Training Accuracy: 0.7012, Validation F1: 0.6559, Validation Accuracy: 0.6560\n",
            "Epoch: 750, Training F1: 0.6896, Training Accuracy: 0.6956, Validation F1: 0.6408, Validation Accuracy: 0.6423\n",
            "Epoch: 800, Training F1: 0.7037, Training Accuracy: 0.7047, Validation F1: 0.6453, Validation Accuracy: 0.6499\n",
            "Epoch: 850, Training F1: 0.6983, Training Accuracy: 0.7020, Validation F1: 0.6540, Validation Accuracy: 0.6545\n",
            "Epoch: 900, Training F1: 0.7085, Training Accuracy: 0.7096, Validation F1: 0.6377, Validation Accuracy: 0.6423\n",
            "Epoch: 950, Training F1: 0.7048, Training Accuracy: 0.7058, Validation F1: 0.6636, Validation Accuracy: 0.6636\n",
            "Epoch: 1000, Training F1: 0.7046, Training Accuracy: 0.7047, Validation F1: 0.6614, Validation Accuracy: 0.6621\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.sigmoid(self.linear(x))\n",
        "        return out\n",
        "\n",
        "# Set up hyperparameters\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoader for mini-batch processing\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create DataLoader for validation dataset\n",
        "validation_dataset = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = LogisticRegression(input_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model and evaluate on validation dataset\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(x)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate and print accuracy on training dataset\n",
        "    # Calculate and print Macro-averaged F1 and accuracy on training dataset\n",
        "    with torch.no_grad():\n",
        "        train_outputs = model(X_train_tensor)\n",
        "        train_preds = (train_outputs > 0.5).float()\n",
        "        train_acc = accuracy_score(y_train_tensor, train_preds)\n",
        "        train_f1 = f1_score(y_train_tensor, train_preds, average='macro')\n",
        "    \n",
        "    # Calculate and print Macro-averaged F1 and accuracy on validation dataset\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "        y_val_true = []\n",
        "        y_val_preds = []\n",
        "\n",
        "        for x_val, y_val in validation_loader:\n",
        "            val_outputs = model(x_val)\n",
        "            val_preds = (val_outputs > 0.5).float()\n",
        "            correct_preds += (val_preds == y_val).sum().item()\n",
        "            total_preds += x_val.size(0)\n",
        "            y_val_true.extend(y_val.tolist())\n",
        "            y_val_preds.extend(val_preds.tolist())\n",
        "\n",
        "        val_acc = correct_preds / total_preds\n",
        "        val_f1 = f1_score(y_val_true, y_val_preds, average='macro')\n",
        "        model.train()\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Training F1: {train_f1:.4f}, Training Accuracy: {train_acc:.4f}, Validation F1: {val_f1:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FNOaF2c1hKH"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDahujpauToh",
        "outputId": "0f2265af-00b0-46f5-f90d-7f41ec9f7f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training F1 (Naive Bayes): 0.4875\n",
            "Training Accuracy (Naive Bayes): 0.5782\n",
            "Validation F1 (Naive Bayes): 0.5335\n",
            "Validation Accuracy (Naive Bayes): 0.5693\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and training accuracy\n",
        "train_preds_NB = naive_bayes_classifier.predict(X_train)\n",
        "train_acc_NB = accuracy_score(y_train, train_preds_NB)\n",
        "train_f1_NB = f1_score(y_train, train_preds_NB, average='macro')\n",
        "print(f\"Training F1 (Naive Bayes): {train_f1_NB:.4f}\")\n",
        "print(f\"Training Accuracy (Naive Bayes): {train_acc_NB:.4f}\")\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and validation accuracy\n",
        "validation_preds_NB = naive_bayes_classifier.predict(X_validation)\n",
        "validation_acc_NB = accuracy_score(y_validation, validation_preds_NB)\n",
        "validation_f1_NB = f1_score(y_validation, validation_preds_NB, average='macro')\n",
        "print(f\"Validation F1 (Naive Bayes): {validation_f1_NB:.4f}\")\n",
        "print(f\"Validation Accuracy (Naive Bayes): {validation_acc_NB:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQgifZ8G4KWV"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJyDbd6q10gW",
        "outputId": "bdcc63f0-9a92-408e-9ae8-55a0ad105d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training F1 (SVM): 0.7427\n",
            "Training Accuracy (SVM): 0.7429\n",
            "Validation F1 (SVM): 0.5251\n",
            "Validation Accuracy (SVM): 0.5784\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', C=1)\n",
        "\n",
        "# Train the SVM classifier using the scaled training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and training accuracy\n",
        "train_preds_svm = svm_classifier.predict(X_train)\n",
        "train_acc_svm = accuracy_score(y_train, train_preds_svm)\n",
        "train_f1_svm = f1_score(y_train, train_preds_svm, average='macro')\n",
        "print(f\"Training F1 (SVM): {train_f1_svm:.4f}\")\n",
        "print(f\"Training Accuracy (SVM): {train_acc_svm:.4f}\")\n",
        "\n",
        "# Calculate and print Macro-averaged F1 and validation accuracy\n",
        "validation_preds_svm = svm_classifier.predict(X_validation)\n",
        "validation_acc_svm = accuracy_score(y_validation, validation_preds_svm)\n",
        "validation_f1_svm = f1_score(y_validation, validation_preds_svm, average='macro')\n",
        "print(f\"Validation F1 (SVM): {validation_f1_svm:.4f}\")\n",
        "print(f\"Validation Accuracy (SVM): {validation_acc_svm:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZL0CVl_E-6E"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyUNQ2f-4Oy_",
        "outputId": "0f2be05c-9eca-4d8f-b282-779fc1c25020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nombUBGVFBEx",
        "outputId": "b07eccb9-f5bb-4467-9409-5833e6ed14cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training F1: 0.7134, Training Accuracy: 0.7111, Validation F1: 0.6576, Validation Accuracy: 0.6522\n",
            "Epoch: 2, Training F1: 0.7287, Training Accuracy: 0.7273, Validation F1: 0.6645, Validation Accuracy: 0.6578\n",
            "Epoch: 3, Training F1: 0.7423, Training Accuracy: 0.7389, Validation F1: 0.6701, Validation Accuracy: 0.6635\n",
            "Epoch: 4, Training F1: 0.7561, Training Accuracy: 0.7488, Validation F1: 0.6764, Validation Accuracy: 0.6694\n",
            "Epoch: 5, Training F1: 0.7694, Training Accuracy: 0.7585, Validation F1: 0.6824, Validation Accuracy: 0.6755\n",
            "Epoch: 6, Training F1: 0.7822, Training Accuracy: 0.7671, Validation F1: 0.6881, Validation Accuracy: 0.6818\n",
            "Epoch: 7, Training F1: 0.7945, Training Accuracy: 0.7748, Validation F1: 0.6936, Validation Accuracy: 0.6883\n",
            "Epoch: 8, Training F1: 0.8063, Training Accuracy: 0.7816, Validation F1: 0.6989, Validation Accuracy: 0.6950\n",
            "Epoch: 9, Training F1: 0.8177, Training Accuracy: 0.7875, Validation F1: 0.7040, Validation Accuracy: 0.7019\n",
            "Epoch: 10, Training F1: 0.8287, Training Accuracy: 0.7926, Validation F1: 0.7089, Validation Accuracy: 0.7089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertConfig\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_data(data, tokenizer, max_length=512):\n",
        "    combined_data = []\n",
        "    for index, row in data.iterrows():\n",
        "        combined_text = row['text'] + ' [SEP] ' + str(row['clickbait'])\n",
        "        combined_data.append(combined_text)\n",
        "    tokenized_data = tokenizer(combined_data, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    return tokenized_data\n",
        "\n",
        "train_encodings = tokenize_data(train_df, tokenizer)\n",
        "validation_encodings = tokenize_data(validation_df, tokenizer)\n",
        "\n",
        "# Create torch dataset\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], torch.tensor(y_train.values))\n",
        "validation_dataset = TensorDataset(validation_encodings[\"input_ids\"], validation_encodings[\"attention_mask\"], torch.tensor(y_validation.values))\n",
        "\n",
        "# Define the configuration for the BERT model\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "config.hidden_dropout_prob = 0.3  # Set the dropout probability\n",
        "\n",
        "# Initialize the BERT model with the specified configuration\n",
        "model = BertForSequenceClassification(config)\n",
        "model.to(\"cuda\")  # Use GPU if available\n",
        "\n",
        "# Set up hyperparameters\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "weight_decay=0.01\n",
        "\n",
        "# Create DataLoader for mini-batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set up the optimizer with weight decay for L2 regularization\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs)\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    training_preds = []\n",
        "    training_labels = []\n",
        "    for i, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Append training predictions and labels\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        training_preds.extend(preds.cpu().numpy())\n",
        "        training_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate training accuracy and F1\n",
        "    training_acc = accuracy_score(training_labels, training_preds)\n",
        "    training_f1 = f1_score(training_labels, training_preds, average='macro')\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    model.eval()\n",
        "    validation_preds = []\n",
        "    validation_labels = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in validation_loader:\n",
        "            input_ids, attention_mask, labels = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            validation_preds.extend(preds.cpu().numpy())\n",
        "            validation_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy and F1\n",
        "    val_acc = accuracy_score(validation_labels, validation_preds)\n",
        "    val_f1 = f1_score(validation_labels, validation_preds, average='macro')\n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}, Training F1: {training_f1:.4f}, Training Accuracy: {training_acc:.4f}, Validation F1: {val_f1:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roberta"
      ],
      "metadata": {
        "id": "3jjtqEOsJ3IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def tokenize_data(data, tokenizer, max_length=128):\n",
        "    combined_data = []\n",
        "    for index, row in data.iterrows():\n",
        "        combined_text = row['text'] + ' [SEP] ' + str(row['clickbait'])\n",
        "        combined_data.append(combined_text)\n",
        "    tokenized_data = tokenizer(combined_data, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    return tokenized_data\n",
        "\n",
        "train_encodings = tokenize_data(train_df, tokenizer)\n",
        "validation_encodings = tokenize_data(validation_df, tokenizer)\n",
        "\n",
        "# Create torch dataset\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], torch.tensor(y_train.values))\n",
        "validation_dataset = TensorDataset(validation_encodings[\"input_ids\"], validation_encodings[\"attention_mask\"], torch.tensor(y_validation.values))\n",
        "\n",
        "# Initialize the Roberta model\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2, hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
        "model.to(\"cuda\")  # Use GPU if available\n",
        "\n",
        "# model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "# model.to(\"cuda\")  # Use GPU if available\n",
        "\n",
        "# Set up hyperparameters\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Create DataLoader for mini-batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set up the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs)\n",
        "\n",
        "# Fine-tune the Roberta model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    training_preds = []\n",
        "    training_labels = []\n",
        "    for i, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Append training predictions and labels\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        training_preds.extend(preds.cpu().numpy())\n",
        "        training_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate training accuracy and F1\n",
        "    training_acc = accuracy_score(training_labels, training_preds)\n",
        "    training_f1 = f1_score(training_labels, training_preds, average='macro')\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    model.eval()\n",
        "    validation_preds = []\n",
        "    validation_labels = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in validation_loader:\n",
        "            input_ids, attention_mask, labels = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            validation_preds.extend(preds.cpu().numpy())\n",
        "            validation_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_acc = accuracy_score(validation_labels, validation_preds)\n",
        "    val_f1 = f1_score(validation_labels, validation_preds, average='macro')\n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}, Training F1: {training_f1:.4f}, Training Accuracy: {training_acc:.4f}, Validation F1: {val_f1:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbSYcTn_Jebc",
        "outputId": "995a9b13-ad49-4e07-dc11-ba9a83ebf3dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training F1: 0.7753, Training Accuracy: 0.7711, Validation F1: 0.7134, Validation Accuracy: 0.7078\n",
            "Epoch: 2, Training F1: 0.7826, Training Accuracy: 0.7774, Validation F1: 0.7178, Validation Accuracy: 0.7121\n",
            "Epoch: 3, Training F1: 0.7898, Training Accuracy: 0.7837, Validation F1: 0.7219, Validation Accuracy: 0.7163\n",
            "Epoch: 4, Training F1: 0.7969, Training Accuracy: 0.7899, Validation F1: 0.7258, Validation Accuracy: 0.7204\n",
            "Epoch: 5, Training F1: 0.8038, Training Accuracy: 0.7959, Validation F1: 0.7295, Validation Accuracy: 0.7243\n",
            "Epoch: 6, Training F1: 0.8105, Training Accuracy: 0.8017, Validation F1: 0.7330, Validation Accuracy: 0.7281\n",
            "Epoch: 7, Training F1: 0.8170, Training Accuracy: 0.8073, Validation F1: 0.7363, Validation Accuracy: 0.7318\n",
            "Epoch: 8, Training F1: 0.8233, Training Accuracy: 0.8127, Validation F1: 0.7394, Validation Accuracy: 0.7354\n",
            "Epoch: 9, Training F1: 0.8294, Training Accuracy: 0.8180, Validation F1: 0.7423, Validation Accuracy: 0.7389\n",
            "Epoch: 10, Training F1: 0.8354, Training Accuracy: 0.8230, Validation F1: 0.7450, Validation Accuracy: 0.7422\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
